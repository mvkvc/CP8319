{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Using Deterministic Environment\n",
      "-------------------------\n",
      "\n",
      "-------------------------\n",
      "Beginning First-Visit Monte Carlo\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 100.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Beginning Temporal-Difference\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 100.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Beginning Q-Learning\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 100.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Using Stochastic Environment\n",
      "-------------------------\n",
      "\n",
      "-------------------------\n",
      "Beginning First-Visit Monte Carlo\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 13.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Beginning Temporal-Difference\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 100.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Beginning Q-Learning\n",
      "-------------------------\n",
      "The success rate of the policy across 500 episodes was 100.00 percent.\n",
      "\n",
      "-------------------------\n",
      "Using Stochastic Environment With Improvements\n",
      "-------------------------\n",
      "\n",
      "-------------------------\n",
      "Beginning Temporal-Difference\n",
      "-------------------------\n",
      "Iteration 1 with accuracy: 100.00\n",
      "Iteration 2 with accuracy: 100.00\n",
      "Iteration 3 with accuracy: 99.00\n",
      "Iteration 4 with accuracy: 100.00\n",
      "Iteration 5 with accuracy: 100.00\n",
      "Iteration 6 with accuracy: 100.00\n",
      "Iteration 7 with accuracy: 100.00\n",
      "Iteration 8 with accuracy: 100.00\n",
      "Iteration 9 with accuracy: 100.00\n",
      "Iteration 10 with accuracy: 100.00\n",
      "The average accuracy is 99.90 with variance 0.09\n",
      "\n",
      "-------------------------\n",
      "Beginning Q-Learning\n",
      "-------------------------\n",
      "Iteration 1 with accuracy: 100.00\n",
      "Iteration 2 with accuracy: 100.00\n",
      "Iteration 3 with accuracy: 100.00\n",
      "Iteration 4 with accuracy: 100.00\n",
      "Iteration 5 with accuracy: 100.00\n",
      "Iteration 6 with accuracy: 100.00\n",
      "Iteration 7 with accuracy: 100.00\n",
      "Iteration 8 with accuracy: 100.00\n",
      "Iteration 9 with accuracy: 100.00\n",
      "Iteration 10 with accuracy: 100.00\n",
      "The average accuracy is 100.00 with variance 0.00\n",
      "\n",
      "Using gamma [0.9, 0.9] and alpha [0.65, 0.9] the resulting var, acc is [99.90, 0.09] for TD and [100.00, 0.00] for Q\n"
     ]
    }
   ],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import json\n",
    "import time\n",
    "from lake_envs import *\n",
    "from tqdm import trange\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "hole_states = [7, 9, 12]  # the states in the environment with holes\n",
    "\n",
    "\n",
    "# %%\n",
    "def sample_action(policy, state):\n",
    "    \"\"\"\n",
    "    Given a stochastic policy (can also be deterministic where only one action has probability 1),\n",
    "    sample an action according to the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        policy: np.ndarray[nS, nA]\n",
    "            The policy to follow for generation of the episode. Since policy can be\n",
    "            policy is a matrix (i.e., 2D array) of size numb_states (nS) x numb_actions (nA).\n",
    "            For example, `policy[0,2]` return the probability of action 2 in state 1. Note that\n",
    "            `np.sum(policy[i]) should sum to 1 for all states. That is the sum of the probabilities of\n",
    "            all actions in a given state (i.e., sum of each row) should sum to 1.\n",
    "        state: int\n",
    "            The state to sample the action from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        action: int\n",
    "            Returns the action that was chosen from the stochastic policy.\n",
    "\n",
    "    \"\"\"\n",
    "    nS, nA = policy.shape\n",
    "    all_actions = np.arange(nA)\n",
    "    return np.random.choice(all_actions, p=policy[state])\n",
    "\n",
    "\n",
    "# %%\n",
    "def take_one_step(env, policy, state):\n",
    "    \"\"\"\n",
    "    This function takes one step in the environment according to the stochastic policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        env: given enviroment, here frozenlake\n",
    "        policy: np.ndarray[nS, nA]\n",
    "            See the description in `sample_action`.\n",
    "        state: int\n",
    "            The current state where the agent is in the environment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        action: int\n",
    "            the action that was chosen from the stochastic policy.\n",
    "        reward: float\n",
    "            the reward that was obtained during this step\n",
    "        new_state: int\n",
    "            the new state that the agent transitioned to\n",
    "        done: boolean\n",
    "            If done is `True` this indicates that we have entered a terminating state\n",
    "            (i.e, `new_state` is a terminating state).\n",
    "\n",
    "    \"\"\"\n",
    "    action = sample_action(policy, state)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    return action, reward, new_state, done\n",
    "\n",
    "\n",
    "# %%\n",
    "def generate_episode(env, policy, max_steps=500):\n",
    "    \"\"\"\n",
    "    Since Monte Carlo methods are based on learning from episodes write a function `random_episode`\n",
    "    that generates an episode given the frozenlake environment and a policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        env: given enviroment, here frozenlake\n",
    "        policy: np.ndarray[nS, nA]\n",
    "            See the description in `sample_action`.\n",
    "        max_steps: int\n",
    "            The maximum number of steps that the episode could take. If a terminating state\n",
    "            is not reached within this time, terminate the episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        episode: list of [(state, action, reward)] triplet.\n",
    "            For example, [(0,1,0),(4,2,0)] indicates that in the first time\n",
    "            step we were in state 0 took action 1 and observed reward 0\n",
    "            (it also means we transitioned to state 4). Similarly, in the\n",
    "            second time step we are in state 4 took action 2 and observed reward 0.\n",
    "\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    curr_state = (\n",
    "        env.reset()\n",
    "    )  # reset the environment and place the agent in the start square\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    num_steps = 0\n",
    "    while num_steps < max_steps:\n",
    "        action, reward, new_state, done = take_one_step(env, policy, curr_state)\n",
    "        episode.append((curr_state, action, reward))\n",
    "        if done:\n",
    "            # print(\"Episode finished after {} timesteps\".format(num_steps + 1))\n",
    "            break\n",
    "        curr_state = new_state\n",
    "        num_steps += 1\n",
    "    ############################\n",
    "    return episode\n",
    "\n",
    "\n",
    "# %%\n",
    "def generate_returns(episode, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Given an episode, generate the total return from each step in the episode based on the\n",
    "    discount factor gamma. For example, let the episode be:\n",
    "    [(0,1,1),(4,2,-1),(6,3,0),(8,0,2)]\n",
    "    and gamma=0.9. Then the total return in the first time step is:\n",
    "    1 + 0.9 * -1 + 0.9^2 * 0 + 0.9^3 * 2\n",
    "    In the second time step it is:\n",
    "    -1 + 0.9 * 0 + 0.9^2 * 2\n",
    "    In the third time step it is:\n",
    "    0 + 0.9 * 2\n",
    "    And finally, in the last time step it is:\n",
    "    2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        episode: list\n",
    "            The episode is assumed to be in the same format as the output of the `generate_episode`\n",
    "            described above.\n",
    "        gamma: float\n",
    "            This is the discount factor, which is a number between 0 and 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        epi_returns: np.ndarray[len(episode)]\n",
    "            The array containing the total returns for each step of the episode.\n",
    "\n",
    "    \"\"\"\n",
    "    len_episode = len(episode)\n",
    "    epi_returns = np.zeros(len_episode)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # HINT: Representing immediate reward as a vector and\n",
    "    # using a vector of powers of gamma along with `np.dot` will\n",
    "    # make this much easier to implement in a few lines of code.\n",
    "    # You don't need to use this approach however and use whatever works for you. #\n",
    "    returns = [j[2] for j in episode]\n",
    "    for step in range(len(epi_returns)):\n",
    "        powers = np.arange(0, len(epi_returns) - step) + 1\n",
    "        gamma_sq = gamma ** powers\n",
    "        step_returns = returns[step:]\n",
    "        epi_returns[step] = np.dot(step_returns, gamma_sq ** powers)\n",
    "    ############################\n",
    "    return epi_returns\n",
    "\n",
    "\n",
    "# %%\n",
    "def mc_policy_evaluation(env, policy, Q_value, n_visits, gamma=0.9):\n",
    "    \"\"\"Update the current Q_values and n_visits by generating one random episode\n",
    "    and using the given policy and the Monte Carlo first-visit approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        env: given enviroment, here frozenlake\n",
    "        policy: np.ndarray[nS, nA]\n",
    "            See the description in `sample_action`.\n",
    "        Q_value: np.ndarray[nS, nA]\n",
    "            The current Q_values. This is a matrix (i.e., 2D array) of size\n",
    "            numb_states (nS) x numb_actions (nA). For example, `Q_value[0, 1]` is the current\n",
    "            estimate of the Q_value for state 0 and action 1.\n",
    "        n_visits: np.ndarray[nS, nA]\n",
    "            The current number of times a (state, action) pair has been visited.\n",
    "            This is a matrix (i.e., 2D array) of size numb_states (nS) x numb_actions (nA).\n",
    "            For example, `n_visits[0, 1]` is the current number of times action 1 has been performed in state 0.\n",
    "        gamma: float\n",
    "            This is the discount factor, which is a number between 0 and 1.\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    nS = env.nS  # number of states\n",
    "    nA = env.nA  # number of actions\n",
    "    episode = generate_episode(env, policy)\n",
    "    returns = generate_returns(episode, gamma=gamma)\n",
    "    visit_flag = np.zeros((nS, nA))\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for step in range(len(episode)):  # For every episode\n",
    "        state = episode[step][0]\n",
    "        action = episode[step][1]\n",
    "        n_visits[state, action] += 1\n",
    "        if visit_flag[state, action] < 1:  # Check if visited before\n",
    "            Q_value[state, action] = Q_value[state, action] + (\n",
    "                1 / n_visits[state, action]\n",
    "            ) * (returns[step] - Q_value[state, action])\n",
    "        visit_flag[state, action] += 1\n",
    "    ############################\n",
    "    return Q_value, n_visits\n",
    "\n",
    "\n",
    "# %%\n",
    "def epsilon_greedy_policy_improve(Q_value, nS, nA, epsilon):\n",
    "    \"\"\"Given the Q_value function and epsilon generate a new epsilon-greedy policy.\n",
    "    IF TWO ACTIONS HAVE THE SAME MAXIMUM Q VALUE, THEY MUST BOTH BE EXECUTED EQUALLY LIKELY.\n",
    "    THIS IS IMPORTANT FOR EXPLORATION.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_value: np.ndarray[nS, nA]\n",
    "        Defined similar to the input of `mc_policy_evaluation`.\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    epsilon: float\n",
    "        current value of epsilon\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS, nA]\n",
    "        The new epsilon-greedy policy according. The shape of the new policy is\n",
    "        as described in `sample_action`.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = epsilon * np.ones((nS, nA)) / nA\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # HINT: IF TWO ACTIONS HAVE THE SAME MAXIMUM Q VALUE, THEY MUST BOTH BE EXECUTED EQUALLY LIKELY.\n",
    "    #     THIS IS IMPORTANT FOR EXPLORATION. This might prove useful:\n",
    "    #     https://stackoverflow.com/questions/17568612/how-to-make-numpy-argmax-return-all-occurrences-of-the-maximum\n",
    "    # np.argwhere(a == np.amax(a)).flatten().tolist())\n",
    "    for state in range(nS):\n",
    "        best_actions = (\n",
    "            np.argwhere(Q_value[state, :] == np.amax(Q_value[state, :]))\n",
    "            .flatten()\n",
    "            .tolist()\n",
    "        )\n",
    "        new_policy[state, best_actions] += (1 - epsilon) / len(best_actions)\n",
    "    ###################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "# %%\n",
    "def mc_glie(env, iterations=1000, gamma=0.9):\n",
    "    \"\"\"This function implements the first-visit Monte Carlo GLIE policy iteration for finding\n",
    "    the optimal policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: given enviroment, here frozenlake\n",
    "    iterations: int\n",
    "        the number of iterations to try\n",
    "    gamma: float\n",
    "        discount factor\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Q_value: np.ndarray[nS, nA]\n",
    "        The Q_value at the end of iterations\n",
    "    det_policy: np.ndarray[nS]\n",
    "        The greedy (i.e., deterministic policy)\n",
    "    \"\"\"\n",
    "    nS = env.nS  # number of states\n",
    "    nA = env.nA  # number of actions\n",
    "    Q_value = np.zeros((nS, nA))\n",
    "    n_visits = np.zeros((nS, nA))\n",
    "    policy = (\n",
    "        np.ones((env.nS, env.nA)) / env.nA\n",
    "    )  # initially all actions are equally likely\n",
    "    epsilon = 1\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # HINT: Don't forget to decay epsilon according to GLIE\n",
    "    # for iter in trange(0, iterations):\n",
    "    for iter in range(0, iterations):\n",
    "        mc_policy_evaluation(env, policy, Q_value, n_visits, gamma)\n",
    "        epsilon = 1 / (iter + 1)\n",
    "        epsilon_greedy_policy_improve(Q_value, nS, nA, epsilon)\n",
    "    ############################\n",
    "    det_policy = np.argmax(Q_value, axis=1)\n",
    "    return Q_value, det_policy\n",
    "\n",
    "\n",
    "# %%\n",
    "def td_sarsa(env, iterations=1000, gamma=0.9, alpha=0.1):\n",
    "    \"\"\"This function implements the temporal-difference SARSA policy iteration for finding\n",
    "    the optimal policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: given enviroment, here frozenlake\n",
    "    iterations: int\n",
    "        the number of iterations to try\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    alpha: float\n",
    "        The learning rate during Q-value updates\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Q_value: np.ndarray[nS, nA]\n",
    "        The Q_value at the end of iterations\n",
    "    det_policy: np.ndarray[nS]\n",
    "        The greedy (i.e., deterministic policy)\n",
    "    \"\"\"\n",
    "\n",
    "    nS = env.nS  # number of states\n",
    "    nA = env.nA  # number of actions\n",
    "    Q_value = np.zeros((nS, nA))\n",
    "    policy = np.ones((env.nS, env.nA)) / env.nA\n",
    "    epsilon = 1\n",
    "    s_t1 = env.reset()  # reset the environment and place the agent in the start square\n",
    "    a_t1 = sample_action(policy, s_t1)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # HINT: Don't forget to decay epsilon according to GLIE\n",
    "    # for iter in trange(0, iterations):\n",
    "    for iter in range(0, iterations):\n",
    "        epsilon = 1 / (iter + 1)\n",
    "        policy = epsilon_greedy_policy_improve(Q_value, nS, nA, epsilon)\n",
    "        state = env.reset()\n",
    "        action = sample_action(policy, s_t1)\n",
    "        done = False\n",
    "        while not done:\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            if new_state in hole_states:\n",
    "                reward = -1\n",
    "            new_action = sample_action(policy, new_state)\n",
    "            Q_value[state, action] = Q_value[state, action] + alpha * (\n",
    "                reward\n",
    "                + gamma * (Q_value[new_state, new_action])\n",
    "                - Q_value[state, action]\n",
    "            )\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "    ############################\n",
    "    det_policy = np.argmax(Q_value, axis=1)\n",
    "    return Q_value, det_policy\n",
    "\n",
    "\n",
    "# epsilon_greedy_policy_improve(Q_value, nS, nA, epsilon)\n",
    "# return new_policy\n",
    "\n",
    "\n",
    "# %%\n",
    "def qlearning(env, iterations=1000, gamma=0.9, alpha=0.1):\n",
    "    \"\"\"This function implements the Q-Learning policy iteration for finding\n",
    "    the optimal policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: given enviroment, here frozenlake\n",
    "    iterations: int\n",
    "        the number of iterations to try\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    alpha: float\n",
    "        The learning rate during Q-value updates\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Q_value: np.ndarray[nS, nA]\n",
    "        The Q_value at the end of iterations\n",
    "    det_policy: np.ndarray[nS]\n",
    "        The greedy (i.e., deterministic policy)\n",
    "    \"\"\"\n",
    "    nS = env.nS  # number of states\n",
    "    nA = env.nA  # number of actions\n",
    "    Q_value = np.zeros((nS, nA))\n",
    "    policy = np.ones((env.nS, env.nA)) / env.nA\n",
    "    epsilon = 1\n",
    "    s_t1 = env.reset()  # reset the environment and place the agent in the start square\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # HINT: Don't forget to decay epsilon according to GLIE\n",
    "    # for iter in trange(0, iterations):\n",
    "    for iter in range(0, iterations):\n",
    "        epsilon = 1 / (iter + 1)\n",
    "        policy = epsilon_greedy_policy_improve(Q_value, nS, nA, epsilon)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = sample_action(policy, state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            if new_state in hole_states:\n",
    "                reward = -1\n",
    "            # new_action = sample_action(policy, new_state)\n",
    "            Q_value[state, action] = Q_value[state, action] + alpha * (\n",
    "                reward + gamma * (np.max(Q_value[new_state])) - Q_value[state, action]\n",
    "            )\n",
    "            state = new_state\n",
    "    ############################\n",
    "    det_policy = np.argmax(Q_value, axis=1)\n",
    "    return Q_value, det_policy\n",
    "\n",
    "\n",
    "# %%\n",
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    env.render()\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# %%\n",
    "def test_performance(env, policy, nb_episodes=500, max_steps=500, output=True):\n",
    "    \"\"\"\n",
    "    This function evaluate the success rate of the policy in reaching\n",
    "    the goal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    nb_episodes: int\n",
    "      number of episodes to evaluate over\n",
    "    max_steps: int\n",
    "      maximum number of steps in each episode\n",
    "    \"\"\"\n",
    "    sum_returns = 0\n",
    "    for i in range(nb_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        for j in range(max_steps):\n",
    "            action = policy[state]\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                sum_returns += reward\n",
    "                break\n",
    "\n",
    "    accuracy = sum_returns / nb_episodes * 100\n",
    "    if output:\n",
    "        print(\n",
    "            \"The success rate of the policy across {} episodes was {:.2f} percent.\".format(\n",
    "                nb_episodes, accuracy\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# %%\n",
    "# Edit below to run the model-free methods on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nUsing Deterministic Environment\\n\" + \"-\" * 25)\n",
    "    time.sleep(0.5)\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    # env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning First-Visit Monte Carlo\\n\" + \"-\" * 25)\n",
    "    Q_mc, policy_mc = mc_glie(env, iterations=1000, gamma=0.9)\n",
    "    test_performance(env, policy_mc)\n",
    "    # render_single(env, policy_mc, 100) # uncomment to see a single episode\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Temporal-Difference\\n\" + \"-\" * 25)\n",
    "    Q_td, policy_td = td_sarsa(env, iterations=1000, gamma=0.9, alpha=0.1)\n",
    "    test_performance(env, policy_td)\n",
    "    # render_single(env, policy_td, 100) # uncomment to see a single episode\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Q-Learning\\n\" + \"-\" * 25)\n",
    "    Q_ql, policy_ql = qlearning(env, iterations=1000, gamma=0.9, alpha=0.1)\n",
    "    test_performance(env, policy_ql)\n",
    "    # render_single(env, policy_ql, 100) # uncomment to see a single episode\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nUsing Stochastic Environment\\n\" + \"-\" * 25)\n",
    "    time.sleep(0.5)\n",
    "    # env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning First-Visit Monte Carlo\\n\" + \"-\" * 25)\n",
    "    Q_mc, policy_mc = mc_glie(env, iterations=1000, gamma=0.9)\n",
    "    test_performance(env, policy_mc)\n",
    "    # render_single(env, policy_mc, 100)  # uncomment to see a single episode\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Temporal-Difference\\n\" + \"-\" * 25)\n",
    "    Q_td, policy_td = td_sarsa(env, iterations=1000, gamma=0.9, alpha=0.1)\n",
    "    test_performance(env, policy_td)\n",
    "    # render_single(env, policy_td, 100)  # uncomment to see a single episode\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Q-Learning\\n\" + \"-\" * 25)\n",
    "    Q_ql, policy_ql = qlearning(env, iterations=1000, gamma=0.9, alpha=0.1)\n",
    "    test_performance(env, policy_ql)\n",
    "    # render_single(env, policy_ql, 100)  # uncomment to see a single episode\n",
    "\n",
    "    print(\n",
    "        \"\\n\"\n",
    "        + \"-\" * 25\n",
    "        + \"\\nUsing Stochastic Environment With Improvements\\n\"\n",
    "        + \"-\" * 25\n",
    "    )\n",
    "    time.sleep(0.5)\n",
    "    env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    def test_parameters(gamma, alpha, iterations=1000, output=True, render=False):\n",
    "        print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Temporal-Difference\\n\" + \"-\" * 25)\n",
    "        td_acc = []\n",
    "        for iter in range(0, 10):\n",
    "            Q_td, policy_td = td_sarsa(env, iterations, gamma[0], alpha[0])\n",
    "            acc = test_performance(env, policy_td, output=False)\n",
    "            td_acc.append(acc)\n",
    "            print(\"Iteration {} with accuracy: {:.2f}\".format(iter + 1, acc))\n",
    "        avg_td = np.mean(td_acc)\n",
    "        var_td = np.var(td_acc)\n",
    "        print(\n",
    "            \"The average accuracy is {:.2f} with variance {:.2f}\".format(avg_td, var_td)\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "        if render:\n",
    "            render_single(env, policy_td, 100)  # uncomment to see a single episode\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Q-Learning\\n\" + \"-\" * 25)\n",
    "        ql_acc = []\n",
    "        for iter in range(0, 10):\n",
    "            Q_ql, policy_ql = qlearning(env, iterations, gamma[1], alpha[1])\n",
    "            acc = test_performance(env, policy_ql, output=False)\n",
    "            ql_acc.append(acc)\n",
    "            print(\"Iteration {} with accuracy: {:.2f}\".format(iter + 1, acc))\n",
    "        avg_ql = np.mean(ql_acc)\n",
    "        var_ql = np.var(ql_acc)\n",
    "        print(\n",
    "            \"The average accuracy is {:.2f} with variance {:.2f}\".format(avg_ql, var_ql)\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "        if render:\n",
    "            render_single(env, policy_td, 100)  # uncomment to see a single episode\n",
    "\n",
    "        if output:\n",
    "            print(\"\")\n",
    "            print(\n",
    "                \"Using gamma {} and alpha {} the resulting var, acc is [{:.2f}, {:.2f}] for TD and [{:.2f}, {:.2f}] for Q\".format(\n",
    "                    gamma, alpha, avg_td, var_td, avg_ql, var_ql\n",
    "                )\n",
    "            )\n",
    "        return avg_td, avg_ql\n",
    "\n",
    "    # iterations = 1000\n",
    "    # # param_g = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    # # param_a = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    # param_g = [0.85, 0.9, 0.95]\n",
    "    # param_a = [0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "    # n_comb = len(param_g) * len(param_a)\n",
    "    # best_params = {\"gamma_td\": [], \"gamma_ql\": [], \"alpha_td\": [], \"alpha_ql\": []}\n",
    "    # best_td = 0\n",
    "    # best_ql = 0\n",
    "    # counter = 0\n",
    "\n",
    "    # test = False\n",
    "    # if test:\n",
    "    #     for gamma in param_g:\n",
    "    #         for alpha in param_a:\n",
    "    #             counter += 1\n",
    "    #             print(\n",
    "    #                 \"Iteration {}/{}  testing {}, alpha {}\".format(\n",
    "    #                     counter, n_comb, gamma, alpha\n",
    "    #                 )\n",
    "    #             )\n",
    "    #             avg_td, avg_ql = test_parameters(\n",
    "    #                 [gamma, gamma], [alpha, alpha], output=False\n",
    "    #             )\n",
    "    #             if avg_td >= best_td:\n",
    "    #                 best_params[\"gamma_td\"].append(gamma)\n",
    "    #                 best_params[\"alpha_td\"].append(alpha)\n",
    "    #                 print(\"New or equal best avg TD result {:.2f}\".format(avg_td))\n",
    "    #                 best_td = avg_td\n",
    "    #             if avg_ql >= best_ql:\n",
    "    #                 best_params[\"gamma_ql\"].append(gamma)\n",
    "    #                 best_params[\"alpha_ql\"].append(alpha)\n",
    "    #                 print(\"New or equal best avg Q  result {:.2f}\".format(avg_ql))\n",
    "    #                 best_ql = avg_ql\n",
    "    #             print(\"\")\n",
    "\n",
    "    #     with open(\"params.txt\", \"w\") as file:\n",
    "    #         file.write(json.dumps(best_params))\n",
    "\n",
    "    _, _ = test_parameters(\n",
    "        [0.9, 0.9],\n",
    "        [0.65, 0.9],\n",
    "        iterations=10000,\n",
    "        # iterations=100000,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
